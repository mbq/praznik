#' Pre-discretised Madelon dataset
#'
#' Madelon is a synthetic data set from the NIPS 2003 feature selection challenge, generated by Isabelle Guyon.
#' It contains 480 irrelevant and 20 relevant features, including 5 informative and 15 redundant.
#' In this version, the originally numerical attributes have been pre-discetised into 10 bins, as well as their names have been altered to reveal 20 relevant features (as identified by the Boruta method).
#' @references \url{https://archive.ics.uci.edu/ml/datasets/Madelon}
"MadelonD"

#' Mutual information maximisation filter
#'
#' Calculates mutual information between all attributes and the decision, then returns top k.
#' @useDynLib praznik, .registration=TRUE
#' @template generic
#' @examples data(MadelonD)
#' MIM(MadelonD$X,MadelonD$Y,20)
#' @export
MIM<-function(X,Y,k=3){
 .Call(C_MIM,X,Y,as.integer(k))->ans
 names(ans)<-c("selection","scores")
 ans$selection<-colnames(X)[ans$selection]
 return(ans)
}

#' Minimal conditional mutual information maximisation filter
#'
#' The method starts with an attribute of a maximal mutual information with the decision \eqn{Y}.
#' Then, it greedily adds attribute \eqn{X} with a maximal value of the following criterion:
#' \deqn{J(X)=\min_{W\in S} I(X;Y|W),}
#' where \eqn{S} is the set of already selected attributes.
#' The method stops either when no initial attribute with positive mutual information with \eqn{Y} can be found, or after \code{k} attributes are found.
#' @note CMIM is identical to the Informative Fragments (IF) method.
#' @references "Fast Binary Feature Selection using Conditional Mutual Information Maximisation" F. Fleuret, JMLR (2004)
#' @references "Object recognition with informative features and linear classification" M. Vidal-Naquet and S. Ullman, IEEE Conference on Computer Vision and Pattern Recognition (2003).
#' @template generic
#' @examples data(MadelonD)
#' CMIM(MadelonD$X,MadelonD$Y,20)
#' @export
CMIM<-function(X,Y,k=3){
 .Call(C_CMIM,X,Y,as.integer(k))->ans
 names(ans)<-c("selection","scores")
 ans$selection<-colnames(X)[ans$selection]
 return(ans)
}

#' Minimum redundancy maximal relevancy filter
#'
#' The method starts with an attribute of a maximal mutual information with the decision \eqn{Y}.
#' Then, it greedily adds attribute \eqn{X} with a maximal value of the following criterion:
#' \deqn{J(X)=I(X;Y)-\frac{1}{|S|}\sum_{W\in S} I(X;W),}
#' where \eqn{S} is the set of already selected attributes.
#' The method stops either when no initial attribute with positive mutual information with \eqn{Y} can be found, or after \code{k} attributes are found.
#' @references "Feature Selection Based on Mutual Information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy" H. Peng et al. IEEE Pattern Analysis and Machine Intelligence (PAMI) (2005)
#' @template generic
#' @examples data(MadelonD)
#' MRMR(MadelonD$X,MadelonD$Y,20)
#' @export
MRMR<-function(X,Y,k=3){
 .Call(C_MRMR,X,Y,as.integer(k))->ans
 names(ans)<-c("selection","scores")
 ans$selection<-colnames(X)[ans$selection]
 return(ans)
}

#' Greedy conditional mutual information maximisation filter
#'
#' In each round, the method selects attribute of a maximal conditional mutual information with the decision given all the previously selected attributes. 
#' It stops either when \code{k} attributes have been selected or when the said criterion reaches zero, whatever comes first.
#' Hence, it may return some attributes but less then \code{k}.
#'
#' @note In theory, most other methods in praznik are approximations of this method -- they often work better, though, as in practice conditional mutual information is hardly estimable.
#' @template generic
#' @export
CMI<-function(X,Y,k=3){
 .Call(C_CMI,X,Y,as.integer(k))->ans
 names(ans)<-c("selection","scores")
 ans$selection<-colnames(X)[ans$selection]
 return(ans)
}

#' Joint mutual information filter
#'
#' The method starts with an attribute of a maximal mutual information with the decision \eqn{Y}.
#' Then, it greedily adds attribute \eqn{X} with a maximal value of the following criterion:
#' \deqn{J(X)=\sum_{W\in S} I(X,W;Y),}
#' where \eqn{S} is the set of already selected attributes.
#' The method stops either when no initial attribute with positive mutual information with \eqn{Y} can be found, or after \code{k} attributes are found.
#'
#' @note \code{\link{DISR}} is a normalised version of JMI; \code{\link{JMIM}} and \code{\link{NJMIM}} are modifications of JMI and DISR in which minimal joint information over already selected attributes is used instead of a sum.
#' @references "Data Visualization and Feature Selection: New Algorithms for Nongaussian Data H. Yang and J. Moody, NIPS (1999)
#' @template generic
#' @examples data(MadelonD)
#' JMI(MadelonD$X,MadelonD$Y,20)
#' @export
JMI<-function(X,Y,k=3){
 .Call(C_JMI,X,Y,as.integer(k))->ans
 names(ans)<-c("selection","scores")
 ans$selection<-colnames(X)[ans$selection]
 return(ans)
}

#' Double input symmetrical relevance filter
#'
#' The method starts with an attribute of a maximal mutual information with the decision \eqn{Y}.
#' Then, it greedily adds attribute \eqn{X} with a maximal value of the following criterion:
#' \deqn{J(X)=\sum_{W\in S} \frac{I(X,W;Y)}{H(X,W,Y)},}
#' where \eqn{S} is the set of already selected attributes.
#' The method stops either when no initial attribute with positive mutual information with \eqn{Y} can be found, or after \code{k} attributes are found.
#'
#' @note DISR is a normalised version of \code{\link{JMI}}; \code{\link{JMIM}} and \code{\link{NJMIM}} are modifications of JMI and DISR in which minimal joint information over already selected attributes is used instead of a sum.
#' @references "On the Use of Variable Complementarity for Feature Selection in Cancer Classification" P. Meyer and G. Bontempi, (2006)
#' @template generic
#' @examples data(MadelonD)
#' DISR(MadelonD$X,MadelonD$Y,20)
#' @export
DISR<-function(X,Y,k=3){
 .Call(C_DISR,X,Y,as.integer(k))->ans
 names(ans)<-c("selection","scores")
 ans$selection<-colnames(X)[ans$selection]
 return(ans)
}

MI<-function(X,Y){
 .Call(C_MI,X,Y,0L)->ans
 names(ans)<-colnames(X)
 return(ans)
}

#' Minimal joint mutual information maximisation filter
#'
#' The method starts with an attribute of a maximal mutual information with the decision \eqn{Y}.
#' Then, it greedily adds attribute \eqn{X} with a maximal value of the following criterion:
#' \deqn{J(X)=\min_{W\in S} I(X,W;Y),}
#' where \eqn{S} is the set of already selected attributes.
#' The method stops either when no initial attribute with positive mutual information with \eqn{Y} can be found, or after \code{k} attributes are found.
#'
#' @note \code{\link{NJMIM}} is a normalised version of JMIM; \code{\link{JMI}} and \code{\link{DISR}} are modifications of JMIM and NJMIM in which a sum of joint information over already selected attributes is used instead of a minimum.
#' @template generic
#' @examples data(MadelonD)
#' JMIM(MadelonD$X,MadelonD$Y,20)
#' @references "Feature selection using Joint Mutual Information Maximisation" M. Bennasar, Y. Hicks and R. Setchi, (2015)
#' @export
JMIM<-function(X,Y,k=3){
 .Call(C_JMIM,X,Y,as.integer(k))->ans
 names(ans)<-c("selection","scores")
 ans$selection<-colnames(X)[ans$selection]
 return(ans)
}

#' Minimal normalised joint mutual information maximisation filter
#'
#' The method starts with an attribute of a maximal mutual information with the decision \eqn{Y}.
#' Then, it greedily adds attribute \eqn{X} with a maximal value of the following criterion:
#' \deqn{J(X)=\min_{W\in S} \frac{I(X,W;Y)}{H(X,W,Y)},}
#' where \eqn{S} is the set of already selected attributes.
#' The method stops either when no initial attribute with positive mutual information with \eqn{Y} can be found, or after \code{k} attributes are found.
#'
#' @note NJMIM is a normalised version of \code{\link{JMIM}}; \code{\link{JMI}} and \code{\link{DISR}} are modifications of JMIM and NJMIM in which a sum of joint information over already selected attributes is used instead of a minimum.
#' @template generic
#' @examples data(MadelonD)
#' NJMIM(MadelonD$X,MadelonD$Y,20)
#' @references "Feature selection using Joint Mutual Information Maximisation" M. Bennasar, Y. Hicks and R. Setchi, (2015)
#' @export
NJMIM<-function(X,Y,k=3){
 .Call(C_NJMIM,X,Y,as.integer(k))->ans
 names(ans)<-c("selection","scores")
 ans$selection<-colnames(X)[ans$selection]
 return(ans)
}

